{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 - Cargamos la matriz de distancias entre features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.99707464699999segundos para cargar\n",
      "(48278, 48278)\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Cargamos la matriz de distancias entre cada par de atributos del dataSet (a partir de los datos normalizados)\n",
    "start = timer()\n",
    "manhattan_dist = pd.read_parquet(\"../allen-molecular/data_mtg/48278/continuous/exon_data_48278_manhattan_attributes_float32.gzip\")\n",
    "end = timer()\n",
    "print(str(end- start) + \"segundos para cargar\")\n",
    "print(manhattan_dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 - Normalizamos las distancias al rango [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "manhattan_dist = scaler.fit_transform(manhattan_dist)\n",
    "manhattan_dist = manhattan_dist.astype(np.float32)\n",
    "manhattan_dist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 - Almacenamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_dist = pd.DataFrame(manhattan_dist)\n",
    "manhattan_dist.columns = manhattan_dist.columns.astype(str)\n",
    "manhattan_dist.to_parquet(\"exon_data_48278_manhattan_attributes_normalized_float32.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Cargamos los datos normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.274559364segundos para cargar\n",
      "(48278, 48278)\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Cargamos la matriz de distancias entre cada par de atributos del dataSet (a partir de los datos normalizados)\n",
    "start = timer()\n",
    "manhattan_dist = pd.read_parquet(\"../../allen-molecular/data_mtg/48278/continuous/exon_data_48278_manhattan_attributes_normalized_float32.gzip\")\n",
    "end = timer()\n",
    "print(str(end- start) + \"segundos para cargar\")\n",
    "print(manhattan_dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Generamos la matriz simetrica (no son simetricos por peque√±os decimales del final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_tri = np.triu(manhattan_dist)\n",
    "upper_tril = np.transpose(upper_tri)\n",
    "manhattan_dist = upper_tri + upper_tril\n",
    "np.fill_diagonal(manhattan_dist, 0)\n",
    "upper_tri=0\n",
    "upper_tril=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Comprobamos que la matriz es simetrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "check_symmetric(manhattan_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Transformamos la matriz de distancias en una matriz de afinidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manhattan_dist = 1 - manhattan_dist # Si lo comento es para probar distancia versus afinidad\n",
    "np.fill_diagonal(manhattan_dist, 0) # Mantenemos 0s en la diagonal por scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Generamos el objeto distancia para Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "manhattan_dist = distance.squareform(manhattan_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Aprendemos el modelo de clustering con k clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.21261928500002segundos para generar\n"
     ]
    }
   ],
   "source": [
    "import scipy.cluster.hierarchy as hclust\n",
    "\n",
    "n_feature_clusters = 100\n",
    "\n",
    "start = timer()\n",
    "hclust_result = hclust.fcluster(hclust.centroid(manhattan_dist), n_feature_clusters, criterion=\"maxclust\")\n",
    "end = timer()\n",
    "\n",
    "print(str(end - start) + \"segundos para generar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hclust_result_manhattan_100_distance_2.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(hclust_result, \"hclust_result_manhattan_100_distance_2.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cargamos el modelo de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "hclust_result = load(\"hclust_result_manhattan_100_distance_2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1 48179]\n",
      " [    2     1]\n",
      " [    3     1]\n",
      " [    4     1]\n",
      " [    5     1]\n",
      " [    6     1]\n",
      " [    7     1]\n",
      " [    8     1]\n",
      " [    9     1]\n",
      " [   10     1]\n",
      " [   11     1]\n",
      " [   12     1]\n",
      " [   13     1]\n",
      " [   14     1]\n",
      " [   15     1]\n",
      " [   16     1]\n",
      " [   17     1]\n",
      " [   18     1]\n",
      " [   19     1]\n",
      " [   20     1]\n",
      " [   21     1]\n",
      " [   22     1]\n",
      " [   23     1]\n",
      " [   24     1]\n",
      " [   25     1]\n",
      " [   26     1]\n",
      " [   27     1]\n",
      " [   28     1]\n",
      " [   29     1]\n",
      " [   30     1]\n",
      " [   31     1]\n",
      " [   32     1]\n",
      " [   33     1]\n",
      " [   34     1]\n",
      " [   35     1]\n",
      " [   36     1]\n",
      " [   37     1]\n",
      " [   38     1]\n",
      " [   39     1]\n",
      " [   40     1]\n",
      " [   41     1]\n",
      " [   42     1]\n",
      " [   43     1]\n",
      " [   44     1]\n",
      " [   45     1]\n",
      " [   46     1]\n",
      " [   47     1]\n",
      " [   48     1]\n",
      " [   49     1]\n",
      " [   50     1]\n",
      " [   51     1]\n",
      " [   52     1]\n",
      " [   53     1]\n",
      " [   54     1]\n",
      " [   55     1]\n",
      " [   56     1]\n",
      " [   57     1]\n",
      " [   58     1]\n",
      " [   59     1]\n",
      " [   60     1]\n",
      " [   61     1]\n",
      " [   62     1]\n",
      " [   63     1]\n",
      " [   64     1]\n",
      " [   65     1]\n",
      " [   66     1]\n",
      " [   67     1]\n",
      " [   68     1]\n",
      " [   69     1]\n",
      " [   70     1]\n",
      " [   71     1]\n",
      " [   72     1]\n",
      " [   73     1]\n",
      " [   74     1]\n",
      " [   75     1]\n",
      " [   76     1]\n",
      " [   77     1]\n",
      " [   78     1]\n",
      " [   79     1]\n",
      " [   80     1]\n",
      " [   81     1]\n",
      " [   82     1]\n",
      " [   83     1]\n",
      " [   84     1]\n",
      " [   85     1]\n",
      " [   86     1]\n",
      " [   87     1]\n",
      " [   88     1]\n",
      " [   89     1]\n",
      " [   90     1]\n",
      " [   91     1]\n",
      " [   92     1]\n",
      " [   93     1]\n",
      " [   94     1]\n",
      " [   95     1]\n",
      " [   96     1]\n",
      " [   97     1]\n",
      " [   98     1]\n",
      " [   99     1]\n",
      " [  100     1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(hclust_result, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Cargamos los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.472053420999998segundos para cargar\n",
      "(15928, 48278)\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Cargamos la matriz de distancias entre cada par de atributos del dataSet (a partir de los datos normalizados)\n",
    "start = timer()\n",
    "data = pd.read_parquet(\"../../allen-molecular/data_mtg/48278/continuous/exon_data_48278.gzip\")\n",
    "end = timer()\n",
    "print(str(end- start) + \"segundos para cargar\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Calculamos las distancias al centroide de los clusters y para cada uno escoger el atributo mas cercano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Organizamos los elementos de cada cluster en listas\n",
    "\n",
    "indices = []\n",
    "for i in range(1, n_feature_clusters + 1):\n",
    "    indices_i = []\n",
    "    for j in range(0, len(hclust_result)):\n",
    "        if hclust_result[j] == i:\n",
    "            indices_i.append(j)\n",
    "    indices.append(indices_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Calculamos el centroide cada cluster\n",
    "selected_columns = []\n",
    "for k in range(0, len(indices)):\n",
    "    indices_k_data = data.drop([ data.columns[i] for i in range(0, len(data.columns)) if i not in indices[k]], axis = 1)\n",
    "    transposed_indices_k_data = indices_k_data.transpose()\n",
    "    mean_value = transposed_indices_k_data.mean()\n",
    "    # Calculamos la distancia de cada elemento\n",
    "    min_dist = sys.float_info.max\n",
    "    index_of_closest_element = -1\n",
    "    for j in range(0, len(transposed_indices_k_data)):\n",
    "        distance_to_mean = distance.cityblock(transposed_indices_k_data.iloc[j], mean_value)\n",
    "        if distance_to_mean < min_dist:\n",
    "            min_dist = distance_to_mean\n",
    "            index_of_closest_element = j\n",
    "\n",
    "    column_of_closest_element = indices_k_data.columns[index_of_closest_element]\n",
    "    selected_columns.append(column_of_closest_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X3324',\n",
       " 'X26038',\n",
       " 'X9456',\n",
       " 'X334',\n",
       " 'X6138',\n",
       " 'X23253',\n",
       " 'X94081',\n",
       " 'X54843',\n",
       " 'X3274',\n",
       " 'X221692',\n",
       " 'X6000',\n",
       " 'X254531',\n",
       " 'X2908',\n",
       " 'X100130155',\n",
       " 'X114088',\n",
       " 'X23230',\n",
       " 'X116966',\n",
       " 'X64062',\n",
       " 'X9645',\n",
       " 'X6262',\n",
       " 'X55814',\n",
       " 'X23613',\n",
       " 'X10147',\n",
       " 'X54904',\n",
       " 'X4898',\n",
       " 'X5411',\n",
       " 'X55704',\n",
       " 'X440270',\n",
       " 'X4897',\n",
       " 'X23077',\n",
       " 'X3535',\n",
       " 'X9284',\n",
       " 'X5310',\n",
       " 'X4297',\n",
       " 'X5297',\n",
       " 'X8499',\n",
       " 'X55904',\n",
       " 'X57035',\n",
       " 'X10905',\n",
       " 'X28978',\n",
       " 'X6134',\n",
       " 'X54460',\n",
       " 'X80205',\n",
       " 'X56853',\n",
       " 'X577',\n",
       " 'X9324',\n",
       " 'X5789',\n",
       " 'X54212',\n",
       " 'X4331',\n",
       " 'X64848',\n",
       " 'X6432',\n",
       " 'X5144',\n",
       " 'X140890',\n",
       " 'X23262',\n",
       " 'X9475',\n",
       " 'X8997',\n",
       " 'X55082',\n",
       " 'X9699',\n",
       " 'X10439',\n",
       " 'X9987',\n",
       " 'X4673',\n",
       " 'X23112',\n",
       " 'X57468',\n",
       " 'X9515',\n",
       " 'X3778',\n",
       " 'X55294',\n",
       " 'X23518',\n",
       " 'X5101',\n",
       " 'X6894',\n",
       " 'X285175',\n",
       " 'X286411',\n",
       " 'X54715',\n",
       " 'X10915',\n",
       " 'X58517',\n",
       " 'X55112',\n",
       " 'X1655',\n",
       " 'X8925',\n",
       " 'X780813',\n",
       " 'X347746',\n",
       " 'X54737',\n",
       " 'X9295',\n",
       " 'X7345',\n",
       " 'X151613',\n",
       " 'X100128906',\n",
       " 'X490',\n",
       " 'X9406',\n",
       " 'X4133',\n",
       " 'X2823',\n",
       " 'X9611',\n",
       " 'X6430',\n",
       " 'X1759',\n",
       " 'X7267',\n",
       " 'X1479',\n",
       " 'X2891',\n",
       " 'X10777',\n",
       " 'X51747',\n",
       " 'X25957',\n",
       " 'X378938',\n",
       " 'X55384',\n",
       " 'X440823']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Filtramos los datos originales con las columnas seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[selected_columns].to_parquet(\"exon_data_48278_hclust100_2.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[selected_columns].to_csv(\"exon_data_48278_hclust100_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
